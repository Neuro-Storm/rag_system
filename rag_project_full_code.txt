
================================================================================
# –§–ê–ô–õ: main.py
================================================================================

import gradio as gr
import os
import shutil
from pathlib import Path
from rag_utils import create_vector_index, load_vector_index, check_files_changed
from config import DOCUMENTS_DIR, SUPPORTED_EXTENSIONS, EMBED_MODEL, VECTOR_DIMENSION
from extractors import get_file_extractor

def process_files(files):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    try:
        Path(DOCUMENTS_DIR).mkdir(exist_ok=True)
        
        for file_path in files:
            file_name = os.path.basename(file_path)
            ext = os.path.splitext(file_name)[1].lower()
            
            if ext in SUPPORTED_EXTENSIONS:
                dest_path = os.path.join(DOCUMENTS_DIR, file_name)
                shutil.copy2(file_path, dest_path)
        
        if check_files_changed():
            create_vector_index()
            return f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤. –ò–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª—ë–Ω."
        return f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤. –ò–Ω–¥–µ–∫—Å –∞–∫—Ç—É–∞–ª–µ–Ω."

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤: {str(e)}"

def answer_query(query):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å"""
    if check_files_changed():
        create_vector_index()
    
    index = load_vector_index()
    if not index:
        return "–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."
    
    query_engine = index.as_query_engine(
        similarity_top_k=3,
        llm=None,
        response_mode="no_text"
    )
    response = query_engine.query(query)
    
    if not response.source_nodes:
        return "‚ùå –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    results = []
    for i, node in enumerate(response.source_nodes, 1):
        file_name = node.metadata.get("file_name", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª")
        text = node.text[:1500] + "..." if len(node.text) > 1500 else node.text
        
        results.append(
            f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç #{i}\n"
            f"üìÑ –§–∞–π–ª: {file_name}\n"
            f"‚≠ê –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {node.score:.4f}\n"
            f"{'-'*50}\n"
            f"{text}\n\n"
        )
    
    return "\n".join(results)

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
with gr.Blocks(title="–õ–æ–∫–∞–ª—å–Ω—ã–π RAG") as demo:
    gr.Markdown("# üìÑ –ú–Ω–æ–≥–æ—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π RAG —Å –ø–æ–∏—Å–∫–æ–º –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º")

    with gr.Tab("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
        file_input = gr.File(
            label=f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(SUPPORTED_EXTENSIONS)}",
            file_types=list(SUPPORTED_EXTENSIONS),
            file_count="multiple"
        )
        process_btn = gr.Button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        process_output = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏", interactive=False)

    with gr.Tab("–ü–æ–∏—Å–∫"):
        query_input = gr.Textbox(label="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å", lines=2)
        query_output = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞", interactive=False)
        search_btn = gr.Button("–ù–∞–π—Ç–∏")

    process_btn.click(process_files, inputs=[file_input], outputs=[process_output])
    search_btn.click(answer_query, inputs=[query_input], outputs=[query_output])

if __name__ == "__main__":
    demo.launch()


================================================================================
# –§–ê–ô–õ: rag_utils.py
================================================================================

import os
import pickle
import hashlib
import faiss
import torch
from pathlib import Path
from llama_index.core import Settings, VectorStoreIndex
from llama_index.core.schema import Document
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from config import DOCUMENTS_DIR, INDEX_DIR, SUPPORTED_EXTENSIONS, EMBED_MODEL, VECTOR_DIMENSION
from extractors import get_file_extractor

# –û—Ç–∫–ª—é—á–∞–µ–º LLM –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
Settings.llm = None
Settings.embed_model = HuggingFaceEmbedding(
    model_name=EMBED_MODEL,
    device="cuda" if torch.cuda.is_available() else "cpu",
    embed_batch_size=2
)
Settings.chunk_size = 512
Settings.chunk_overlap = 20

STATE_FILE = Path(INDEX_DIR) / ".file_state.pkl"
FILE_EXTRACTOR = get_file_extractor()

def ensure_directories():
    Path(DOCUMENTS_DIR).mkdir(parents=True, exist_ok=True)
    Path(INDEX_DIR).mkdir(parents=True, exist_ok=True)

def calculate_file_hash(path: str) -> str:
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

def save_file_state(file_hashes: dict):
    with open(STATE_FILE, "wb") as f:
        pickle.dump({
            "timestamp": os.path.getmtime(STATE_FILE) if STATE_FILE.exists() else 0,
            "file_hashes": file_hashes
        }, f)

def load_file_state():
    if not STATE_FILE.exists():
        return {}
    with open(STATE_FILE, "rb") as f:
        return pickle.load(f).get("file_hashes", {})

def check_files_changed() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
    ensure_directories()
    current_hashes = {}
    
    for file in Path(DOCUMENTS_DIR).iterdir():
        if file.is_file() and file.suffix.lower() in SUPPORTED_EXTENSIONS:
            current_hashes[file.name] = calculate_file_hash(file)
    
    saved_hashes = load_file_state()
    return current_hashes != saved_hashes

def create_vector_index():
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        ensure_directories()
        documents = []
        
        # –ß—Ç–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for file_path in Path(DOCUMENTS_DIR).iterdir():
            if not file_path.is_file() or file_path.suffix.lower() not in SUPPORTED_EXTENSIONS:
                continue
            
            extractor = FILE_EXTRACTOR.get(file_path.suffix.lower())
            if extractor:
                try:
                    doc_content = extractor(str(file_path))
                    documents.append(Document(
                        text=doc_content,
                        metadata={"file_name": file_path.name}
                    ))
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {str(e)}")
        
        if not documents:
            print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
            return None
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        pipeline = IngestionPipeline(
            transformations=[
                SentenceSplitter(chunk_size=Settings.chunk_size, chunk_overlap=Settings.chunk_overlap)
            ]
        )
        nodes = pipeline.run(documents=documents)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        faiss_index = faiss.IndexFlatL2(VECTOR_DIMENSION)
        vector_store = FaissVectorStore(faiss_index=faiss_index)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        index = VectorStoreIndex(nodes, storage_context=storage_context)
        index.storage_context.persist(persist_dir=INDEX_DIR)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        save_file_state({
            file.name: calculate_file_hash(file) 
            for file in Path(DOCUMENTS_DIR).iterdir() 
            if file.is_file()
        })
        
        print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.")
        return index

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
        return None

def load_vector_index():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
    try:
        ensure_directories()
        
        if not any(Path(INDEX_DIR).iterdir()):
            return None
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞
        faiss_index = faiss.IndexFlatL2(VECTOR_DIMENSION)
        vector_store = FaissVectorStore.from_persist_dir(INDEX_DIR)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            persist_dir=INDEX_DIR
        )
        
        index = load_index_from_storage(storage_context)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        return index

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
        return None


================================================================================
# –§–ê–ô–õ: config.py
================================================================================

import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCUMENTS_DIR = os.path.join(BASE_DIR, "documents")
INDEX_DIR = os.path.join(BASE_DIR, "index_store")

SUPPORTED_EXTENSIONS = {
    ".pdf", ".txt", ".docx", ".pptx", 
    ".xlsx", ".csv", ".jpg", ".png", 
    ".jpeg", ".html", ".md", ".djvu"
}

EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"
VECTOR_DIMENSION = 1024


================================================================================
# –ü–ê–ü–ö–ê EXTRACTORS
================================================================================


------------------------------------------------------------
# –§–∞–π–ª: extractors/__init__.py
------------------------------------------------------------

from .common import extract_text_common
from .pdf_extractor import extract_text_from_pdf
from .txt_extractor import extract_text_from_txt
from .docx_extractor import extract_text_from_docx
from .pptx_extractor import extract_text_from_pptx
from .xlsx_extractor import extract_text_from_xlsx
from .djvu_extractor import extract_text_from_djvu
from .image_extractor import extract_text_from_image

def get_file_extractor():
    return {
        ".pdf": extract_text_from_pdf,
        ".txt": extract_text_from_txt,
        ".docx": extract_text_from_docx,
        ".pptx": extract_text_from_pptx,
        ".xlsx": extract_text_from_xlsx,
        ".djvu": extract_text_from_djvu,
        ".jpg": extract_text_from_image,
        ".jpeg": extract_text_from_image,
        ".png": extract_text_from_image,
        ".html": extract_text_common,
        ".md": extract_text_common,
        ".csv": extract_text_common
    }


------------------------------------------------------------
# –§–∞–π–ª: extractors/common.py
------------------------------------------------------------

def extract_text_common(file_path: str) -> str:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {str(e)}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/djvu_extractor.py
------------------------------------------------------------

import subprocess

def extract_text_from_djvu(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DJVU —Å –ø–æ–º–æ—â—å—é djvutxt"""
    try:
        result = subprocess.run(
            ["djvutxt", file_path],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )
        return result.stdout
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ DJVU: {str(e)}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/docx_extractor.py
------------------------------------------------------------

from docx import Document

def extract_text_from_docx(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX"""
    try:
        doc = Document(file_path)
        return "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX: {str(e)}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/image_extractor.py
------------------------------------------------------------

from PIL import Image
import pytesseract

def extract_text_from_image(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é Tesseract"""
    try:
        img = Image.open(file_path)
        return pytesseract.image_to_string(img, lang='rus+eng')
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ OCR: {str(e)}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/pdf_extractor.py
------------------------------------------------------------

from PyPDF2 import PdfReader

def extract_text_from_pdf(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            raw_text = page.extract_text()
            if isinstance(raw_text, bytes):
                text += raw_text.decode('utf-8', errors='replace') + "\n"
            else:
                text += raw_text + "\n"
        return text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ PDF: {e}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/pptx_extractor.py
------------------------------------------------------------

from pptx import Presentation

def extract_text_from_pptx(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PPTX"""
    try:
        prs = Presentation(file_path)
        text = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text.append(shape.text)
        return "\n".join(text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PPTX: {str(e)}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/txt_extractor.py
------------------------------------------------------------

def extract_text_from_txt(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ TXT"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ TXT: {e}")
        return ""


------------------------------------------------------------
# –§–∞–π–ª: extractors/xlsx_extractor.py
------------------------------------------------------------

import pandas as pd

def extract_text_from_xlsx(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Excel"""
    try:
        text = []
        xl = pd.ExcelFile(file_path)
        for sheet_name in xl.sheet_names:
            df = xl.parse(sheet_name)
            text.append(df.to_string())
        return "\n".join(text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Excel: {str(e)}")
        return ""

